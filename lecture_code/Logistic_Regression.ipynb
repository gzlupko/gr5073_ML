{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mELOi-lUIMF1"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sG77NaXJIMF-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0917d701-d7f6-409b-e100-445b42058fd6"
      },
      "source": [
        "#Import some example data\n",
        "\n",
        "import pandas as pd\n",
        "# target = InMichelin, whether or not a restaurant is in the Michelin guide\n",
        "data = pd.read_csv(\"http://gattonweb.uky.edu/sheather/book/docs/datasets/MichelinNY.csv\", encoding=\"latin_1\")\n",
        "print(data.head())\n",
        "\n",
        "#update data to set up for train test split\n",
        "data = data.loc[:, data.columns != 'Restaurant Name']\n",
        "y = data['InMichelin']\n",
        "X = data.loc[:, data.columns != 'InMichelin']"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   InMichelin Restaurant Name  Food  Decor  Service  Price\n",
            "0           0  14 Wall Street    19     20       19     50\n",
            "1           0             212    17     17       16     43\n",
            "2           0        26 Seats    23     17       21     35\n",
            "3           1              44    19     23       16     52\n",
            "4           0               A    23     12       19     24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M387P_MHIMGV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a3abc07-1ac2-4c75-f173-7e8f81543840"
      },
      "source": [
        "#Set up training and test data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, random_state=42) \n",
        "\n",
        "#Note: random_state ensures same data will be generated for example each time\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logreg = LogisticRegression(penalty='none').fit(X_train, y_train)\n",
        "\n",
        "print(\"logreg .coef_: {}\".format(logreg .coef_))\n",
        "\n",
        "\n",
        "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
        "print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\n",
        "\n",
        "\n",
        "predicted_vals = logreg.predict(X_test) # y_pred includes your predictions\n",
        "print(\"logreg.predict: {}\".format(predicted_vals))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logreg .coef_: [[ 0.38181614  0.07433425 -0.15691054  0.08189853]]\n",
            "Training set score: 0.797\n",
            "Test set score: 0.780\n",
            "logreg.predict: [0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0\n",
            " 1 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ycvi2emNIMGy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b7ca300-bb18-425f-fee7-c71697ddb627"
      },
      "source": [
        "logreg\n",
        "\n",
        "#Use ?LogisticRegression() for more information"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(penalty='none')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fODA7tsxIMHI"
      },
      "source": [
        "## Logistic Regression in statsmodels package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igNMntjKIMHN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "7e055257-f3e2-4646-b8a4-3344afd57db7"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "X_train_new = sm.add_constant(X_train)\n",
        "\n",
        "model = sm.GLM(y_train, X_train_new, family=sm.families.Binomial()).fit()\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>Generalized Linear Model Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>      <td>InMichelin</td>    <th>  No. Observations:  </th>  <td>   123</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>  <td>   118</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model Family:</th>        <td>Binomial</td>     <th>  Df Model:          </th>  <td>     4</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Link Function:</th>         <td>logit</td>      <th>  Scale:             </th> <td>  1.0000</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -57.266</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>            <td>Mon, 17 May 2021</td> <th>  Deviance:          </th> <td>  114.53</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                <td>20:51:01</td>     <th>  Pearson chi2:      </th>  <td>  254.</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>No. Iterations:</th>          <td>6</td>        <th>                     </th>     <td> </td>   \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "     <td></td>        <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>const</th>   <td>  -10.6490</td> <td>    2.588</td> <td>   -4.115</td> <td> 0.000</td> <td>  -15.722</td> <td>   -5.576</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Food</th>    <td>    0.3818</td> <td>    0.148</td> <td>    2.572</td> <td> 0.010</td> <td>    0.091</td> <td>    0.673</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Decor</th>   <td>    0.0743</td> <td>    0.103</td> <td>    0.720</td> <td> 0.471</td> <td>   -0.128</td> <td>    0.277</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Service</th> <td>   -0.1569</td> <td>    0.147</td> <td>   -1.070</td> <td> 0.285</td> <td>   -0.444</td> <td>    0.131</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Price</th>   <td>    0.0819</td> <td>    0.036</td> <td>    2.269</td> <td> 0.023</td> <td>    0.011</td> <td>    0.153</td>\n",
              "</tr>\n",
              "</table>"
            ],
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                 Generalized Linear Model Regression Results                  \n",
              "==============================================================================\n",
              "Dep. Variable:             InMichelin   No. Observations:                  123\n",
              "Model:                            GLM   Df Residuals:                      118\n",
              "Model Family:                Binomial   Df Model:                            4\n",
              "Link Function:                  logit   Scale:                          1.0000\n",
              "Method:                          IRLS   Log-Likelihood:                -57.266\n",
              "Date:                Mon, 17 May 2021   Deviance:                       114.53\n",
              "Time:                        20:51:01   Pearson chi2:                     254.\n",
              "No. Iterations:                     6                                         \n",
              "Covariance Type:            nonrobust                                         \n",
              "==============================================================================\n",
              "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
              "------------------------------------------------------------------------------\n",
              "const        -10.6490      2.588     -4.115      0.000     -15.722      -5.576\n",
              "Food           0.3818      0.148      2.572      0.010       0.091       0.673\n",
              "Decor          0.0743      0.103      0.720      0.471      -0.128       0.277\n",
              "Service       -0.1569      0.147     -1.070      0.285      -0.444       0.131\n",
              "Price          0.0819      0.036      2.269      0.023       0.011       0.153\n",
              "==============================================================================\n",
              "\"\"\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_VyFy5-IMHc"
      },
      "source": [
        "## Logistic Regression with constraints on size of coefficients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kW-yfnUIMHf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40c89905-2503-45c7-9563-015853d7c9e0"
      },
      "source": [
        "# Smaller C will constrain Betas more.  It's a tuning parameter we can find using gridsearch.\n",
        "\n",
        "#C=100, compare coefs to regular model above.\n",
        "logreg = LogisticRegression(C=100, penalty='l2').fit(X_train, y_train) \n",
        "\n",
        "print(\"logreg .coef_: {}\".format(logreg .coef_))\n",
        "\n",
        "\n",
        "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
        "print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\n",
        "\n",
        "\n",
        "predicted_vals = logreg.predict(X_test) # y_pred includes your predictions\n",
        "print(\"logreg.predict: {}\".format(predicted_vals))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logreg .coef_: [[ 0.38171368  0.07433904 -0.15682846  0.08189077]]\n",
            "Training set score: 0.797\n",
            "Test set score: 0.780\n",
            "logreg.predict: [0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0\n",
            " 1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RCMm5WCIMHq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ecf5963-8d61-4fbf-8e33-6e03fd1b0b1e"
      },
      "source": [
        "\n",
        "#C=1, compare coefs to above models.\n",
        "logreg = LogisticRegression(C=1, penalty='l2').fit(X_train, y_train)\n",
        "\n",
        "print(\"logreg .coef_: {}\".format(logreg .coef_))\n",
        "\n",
        "\n",
        "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
        "print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\n",
        "\n",
        "\n",
        "predicted_vals = logreg.predict(X_test) # y_pred includes your predictions\n",
        "print(\"logreg.predict: {}\".format(predicted_vals))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logreg .coef_: [[ 0.37187726  0.07490079 -0.14897911  0.08113593]]\n",
            "Training set score: 0.797\n",
            "Test set score: 0.780\n",
            "logreg.predict: [0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0\n",
            " 1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpGBOiXCIMIf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66f91a76-9de7-4d3e-94b8-95f7e1fbbd32"
      },
      "source": [
        "\n",
        "#C=.0001, compare coefs to above models.\n",
        "\n",
        "#Does the model's prediction power get better or worse??\n",
        "\n",
        "logreg = LogisticRegression(C=.0001, penalty='l2').fit(X_train, y_train)\n",
        "\n",
        "print(\"logreg .coef_: {}\".format(logreg .coef_))\n",
        "\n",
        "\n",
        "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
        "print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\n",
        "\n",
        "\n",
        "predicted_vals = logreg.predict(X_test) # y_pred includes your predictions\n",
        "print(\"logreg.predict: {}\".format(predicted_vals))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logreg .coef_: [[0.00549429 0.00672568 0.00502413 0.02866617]]\n",
            "Training set score: 0.699\n",
            "Test set score: 0.732\n",
            "logreg.predict: [0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0\n",
            " 1 1 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJlOnuHVnZJd",
        "outputId": "d167fe9b-d7b0-43bb-8301-18ea5dcdb4f0"
      },
      "source": [
        "#What if we want to use an l1 penalty instead?  Change penalty to 'l1' and solver to 'liblinear'.\n",
        "\n",
        "#Does the model's prediction power get better or worse??\n",
        "\n",
        "logreg = LogisticRegression(C=.01, penalty='l1',solver='liblinear').fit(X_train, y_train)\n",
        "\n",
        "print(\"logreg .coef_: {}\".format(logreg .coef_))\n",
        "\n",
        "\n",
        "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
        "print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\n",
        "\n",
        "\n",
        "predicted_vals = logreg.predict(X_test) # y_pred includes your predictions\n",
        "print(\"logreg.predict: {}\".format(predicted_vals))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logreg .coef_: [[-0.02290097  0.          0.          0.00967796]]\n",
            "Training set score: 0.699\n",
            "Test set score: 0.732\n",
            "logreg.predict: [0 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 1 0\n",
            " 1 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FT3rYS9jIMIn"
      },
      "source": [
        "## Multiclass models (Multinomial model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1JVt7blIMIp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "834a6922-0092-4f1d-ac32-13c96358196c"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "iris\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "print(iris.feature_names )# X variable names\n",
        "print(X[0:5]) # first five rows of data\n",
        "\n",
        "print(iris.target_names) #target categories\n",
        "print(np.unique(y)) #target values\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
            "[[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]]\n",
            "['setosa' 'versicolor' 'virginica']\n",
            "[0 1 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuMM3GkSIMIv"
      },
      "source": [
        "logreg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\",max_iter=10000,).fit(X,y) #Note the argument changes to LogisticRegression()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAjgz06fIMI3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1568c278-e1ba-41c3-b170-459ad95a7981"
      },
      "source": [
        "print(logreg.predict(X)) #uses softmax function to predict new X data, but I am being lazy and using X data here."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1\n",
            " 1 1 1 2 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "037ICwrGIMI_"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}