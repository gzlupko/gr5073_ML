{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4\n",
    "### Text Analysis and Neural Networks\n",
    "Gian Zlupko "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part I: Build a classification model using text data\n",
    "\n",
    "#### *Import the text data, vectorize the clickbait headline column into an X matrix.  Then run logistic regression at least three times and select a single best model.  Note that you should create three logistic regression models with different different tokenization approaches.  You should not change your modeling approach, you should simply experiment with different tokenizers. Be sure to explain your choices and evaluate your models using cross validation and using test set data.* \n",
    "\n",
    "First, I import the data below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    0\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: clickbait, dtype: uint8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       MyBook Disk Drive Handles Lots of Easy Backups\n",
       "1                       CIT Posts Eighth Loss in a Row\n",
       "2    Candy Carson Singing The \"National Anthem\" Is ...\n",
       "3    Why You Need To Stop What You're Doing And Dat...\n",
       "4    27 Times Adele Proved She's Actually The Reale...\n",
       "Name: headline, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "\n",
    "# change working directory \n",
    "os.chdir('/Users/gianzlupko/Desktop/GR5073 ML/gr5073_ML/data')\n",
    "\n",
    "# import the data\n",
    "X = pd.read_csv(\"X_train.csv\", squeeze=True)\n",
    "y_labels = pd.read_csv(\"y_train.csv\", squeeze=True)\n",
    "\n",
    "# one hot encode the y data \n",
    "y =  pd.get_dummies(y_labels)\n",
    "y = y.iloc[:, 0] # select only first column (note: clickbait = 1)\n",
    "\n",
    "# inspect the data sets \n",
    "print(y.head()) \n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, after loading the data, I will try the first of three separate tokenization strategies. For the first strategy, I will simply tokenize the text data into word tokens, representing the most simple of the three strategies that I will use to compare.\n",
    "\n",
    "#### Model I: Unigram Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_tokens:\n",
      "<24979x20332 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 220242 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer().fit(X)\n",
    "X_tokens = vect.transform(X) # name this data set 'X_tokens' so that it does not overwrite the original raw X data \n",
    "print(\"X_tokens:\\n{}\".format(repr(X_tokens))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that in the overall data set, there were 220,242 words. I used the default settings for the `CountVectorizer` function, so results from tokenization returned a sparse data matrix with token counts. \n",
    "\n",
    "Next, I follow a standard ML approach to tuning and fitting a logistic regression model on the matrix of token counts. I use grid search CV to tune the hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.97\n",
      "Best parameters:  {'C': 10}\n"
     ]
    }
   ],
   "source": [
    "# Set up training and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# train test split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tokens, y, random_state=42)\n",
    "\n",
    "# fit and tune model using grid search CV \n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "grid = GridSearchCV(LogisticRegression(solver = 'liblinear'), param_grid, cv=5, scoring = 'f1')\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results indicated that the logistic regression model achieved an F1 score of 0.97. This will be the F1 score to beat in the subsequent rounds of testing out different tokenization strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next tokenization strategy that I will test is to extract both unigrams (which we did above) as well as bigrams. The result data matrix will be much wider and larger. \n",
    "\n",
    "#### Model II - Unigrams and Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_tokens:\n",
      "<24979x135950 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 418238 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vect_grams = CountVectorizer(ngram_range = (1,2)).fit(X)\n",
    "X_grams = vect_grams.transform(X) # name this data set 'X_tokens' so that it does not overwrite the original raw X data \n",
    "print(\"X_tokens:\\n{}\".format(repr(X_grams))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, after tokenizing with unigrams and bigrams, we see that the data matrix is even larger than before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.97\n",
      "Best parameters:  {'C': 10}\n"
     ]
    }
   ],
   "source": [
    "# train test split on the new data\n",
    "X_grams_train, X_grams_test, y_grams_train, y_grams_test = train_test_split(X_grams, y)\n",
    "\n",
    "# fit and tune hyperparameters \n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "grid = GridSearchCV(LogisticRegression(solver = 'liblinear'), param_grid, cv=5, scoring = 'f1')\n",
    "grid.fit(X_grams_train, y_grams_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model achieved the same F1 score on the unigram and bigram tokenized data. It's possible this tokenization strategy did not add much signal. While some meaningful phrases and important word co-occurences were likely captured by the use of bigrams in the current step, it evidently did not offer additional explanatory power for the model. \n",
    "\n",
    "Next, I will try one more tokenization strategy to see if I can improve model fit. \n",
    "\n",
    "#### Model III: Unigram Tokenization with TF-IDF and Stop Word Removal \n",
    "\n",
    "For my third attempt, I use TF-IDF to rescale the data and I remove stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.95\n",
      "Best parameters:  {'C': 10}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "vect_scaled = TfidfVectorizer(stop_words = \"english\").fit(X)\n",
    "X_scaled = vect_scaled.transform(X)\n",
    "\n",
    "X_scaled_train, X__scaled_test, y_scaled_train, y_scaled_test = train_test_split(X_scaled, y, random_state=42)\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "grid = GridSearchCV(LogisticRegression(solver  = \"liblinear\", max_iter = 10000), param_grid, cv=5, scoring = \"f1\")\n",
    "grid.fit(X_scaled_train, y_scaled_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that rescaling the data matrix using TF-IDF while also removing stop words, together, did not improve the model's F1 score. This combination of preprocessing and tokenization methods performed the worst of the three attempts. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
