{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4\n",
    "### Text Analysis and Neural Networks\n",
    "Gian Zlupko "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part I: Build a classification model using text data\n",
    "\n",
    "#### *Import the text data, vectorize the clickbait headline column into an X matrix.  Then run logistic regression at least three times and select a single best model.  Note that you should create three logistic regression models with different different tokenization approaches.  You should not change your modeling approach, you should simply experiment with different tokenizers. Be sure to explain your choices and evaluate your models using cross validation and using test set data.* \n",
    "\n",
    "First, I import the data below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    0\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: clickbait, dtype: uint8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       MyBook Disk Drive Handles Lots of Easy Backups\n",
       "1                       CIT Posts Eighth Loss in a Row\n",
       "2    Candy Carson Singing The \"National Anthem\" Is ...\n",
       "3    Why You Need To Stop What You're Doing And Dat...\n",
       "4    27 Times Adele Proved She's Actually The Reale...\n",
       "Name: headline, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "\n",
    "# change working directory \n",
    "os.chdir('/Users/gianzlupko/Desktop/GR5073 ML/gr5073_ML/data')\n",
    "\n",
    "# import the data\n",
    "X = pd.read_csv(\"X_train.csv\", squeeze=True)\n",
    "y_labels = pd.read_csv(\"y_train.csv\", squeeze=True)\n",
    "\n",
    "# one hot encode the y data \n",
    "y =  pd.get_dummies(y_labels)\n",
    "y = y.iloc[:, 0] # select only first column (note: clickbait = 1)\n",
    "\n",
    "# inspect the data sets \n",
    "print(y.head()) \n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, after loading the data, I will try the first of three separate tokenization strategies. For the first strategy, I will simply tokenize the text data into word tokens, representing the most simple of the three strategies that I will use to compare.\n",
    "\n",
    "#### Model I: Unigram Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_tokens:\n",
      "<24979x20332 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 220242 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer().fit(X)\n",
    "X_tokens = vect.transform(X) # name this data set 'X_tokens' so that it does not overwrite the original raw X data \n",
    "print(\"X_tokens:\\n{}\".format(repr(X_tokens))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that in the overall data set, there were 220,242 words. I used the default settings for the `CountVectorizer` function, so results from tokenization returned a sparse data matrix with token counts. \n",
    "\n",
    "Next, I follow a standard ML approach to tuning and fitting a logistic regression model on the matrix of token counts. I use grid search CV to tune the hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.97\n",
      "Best parameters:  {'C': 10}\n"
     ]
    }
   ],
   "source": [
    "# Set up training and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# train test split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tokens, y, random_state=42)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# fit and tune model using grid search CV \n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "grid = GridSearchCV(LogisticRegression(solver = 'liblinear'), param_grid, cv=5, scoring = 'f1')\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results indicated that the logistic regression model achieved an F1 score of 0.97. This will be the F1 score to beat in the subsequent rounds of testing out different tokenization strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next tokenization strategy that I will test is to extract both unigrams (which we did above) as well as bigrams. The result data matrix will be much wider and larger. \n",
    "\n",
    "#### Model II - Unigrams and Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
