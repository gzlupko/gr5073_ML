{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4\n",
    "### Text Analysis and Neural Networks\n",
    "Gian Zlupko "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part I: Build a classification model using text data\n",
    "\n",
    "#### *Import the text data, vectorize the clickbait headline column into an X matrix.  Then run logistic regression at least three times and select a single best model.  Note that you should create three logistic regression models with different different tokenization approaches.  You should not change your modeling approach, you should simply experiment with different tokenizers. Be sure to explain your choices and evaluate your models using cross validation and using test set data.* \n",
    "\n",
    "First, I import the data below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    0\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: clickbait, dtype: uint8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       MyBook Disk Drive Handles Lots of Easy Backups\n",
       "1                       CIT Posts Eighth Loss in a Row\n",
       "2    Candy Carson Singing The \"National Anthem\" Is ...\n",
       "3    Why You Need To Stop What You're Doing And Dat...\n",
       "4    27 Times Adele Proved She's Actually The Reale...\n",
       "Name: headline, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "\n",
    "# change working directory \n",
    "os.chdir('/Users/gianzlupko/Desktop/GR5073 ML/gr5073_ML/data')\n",
    "\n",
    "# import the data\n",
    "X = pd.read_csv(\"X_train.csv\", squeeze=True)\n",
    "y_labels = pd.read_csv(\"y_train.csv\", squeeze=True)\n",
    "\n",
    "# one hot encode the y data \n",
    "y =  pd.get_dummies(y_labels)\n",
    "y = y.iloc[:, 0] # select only first column (note: clickbait = 1)\n",
    "\n",
    "# inspect the data sets \n",
    "print(y.head()) \n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, after loading the data, I will try the first of three separate tokenization strategies. For the first strategy, I will simply tokenize the text data into word tokens, representing the most simple of the three strategies that I will use to compare.\n",
    "\n",
    "#### Model I: Unigram Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_tokens:\n",
      "<24979x20332 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 220242 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer().fit(X)\n",
    "X_tokens = vect.transform(X) # name this data set 'X_tokens' so that it does not overwrite the original raw X data \n",
    "print(\"X_tokens:\\n{}\".format(repr(X_tokens))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that in the overall data set, there were 220,242 words. I used the default settings for the `CountVectorizer` function, so results from tokenization returned a sparse data matrix with token counts. \n",
    "\n",
    "Next, I follow a standard ML approach to tuning and fitting a logistic regression model on the matrix of token counts. I use grid search CV to tune the hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.97\n",
      "Best parameters:  {'C': 10}\n"
     ]
    }
   ],
   "source": [
    "# Set up training and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# train test split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tokens, y, random_state=42)\n",
    "\n",
    "# fit and tune model using grid search CV \n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "grid = GridSearchCV(LogisticRegression(solver = 'liblinear'), param_grid, cv=5, scoring = 'f1')\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results indicated that the logistic regression model achieved an F1 score of 0.97. This will be the F1 score to beat in the subsequent rounds of testing out different tokenization strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next tokenization strategy that I will test is to extract bigrams only. \n",
    "\n",
    "#### Model II - Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_tokens:\n",
      "<24979x135950 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 418238 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vect_bigrams = CountVectorizer(ngram_range = (1,2)).fit(X)\n",
    "X_bigrams = vect_bigrams.transform(X) # name this data set 'X_tokens' so that it does not overwrite the original raw X data \n",
    "print(\"X_tokens:\\n{}\".format(repr(X_bigrams))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, after tokenizing with bigrams, we see that the matrix is larger as now there are more possible combinations for words than there were individuals words alone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.97\n",
      "Best parameters:  {'C': 10}\n"
     ]
    }
   ],
   "source": [
    "# train test split on the new data\n",
    "X_bigrams_train, X_bigrams_test, y_bigrams_train, y_bigrams_test = train_test_split(X_bigrams, y)\n",
    "\n",
    "# fit and tune hyperparameters \n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "grid = GridSearchCV(LogisticRegression(solver = 'liblinear'), param_grid, cv=5, scoring = 'f1')\n",
    "grid.fit(X_bigrams_train, y_bigrams_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model achieved the same F1 score using bigram tokenization as it did with unigram tokenization.\n",
    "\n",
    "Next, I will try one more tokenization strategy to see if I can improve model fit. \n",
    "\n",
    "#### Model III: N-gram Tokenization with TF-IDF and Stop Word Removal \n",
    "\n",
    "For my third attempt, I will extract multiple n-grams (uni to tri-grams). In addition, I will also use TF-IDF to rescale the data and remove stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.95\n",
      "Best parameters:  {'C': 10}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "vect_scaled = TfidfVectorizer(stop_words = \"english\").fit(X)\n",
    "X_scaled = vect_scaled.transform(X)\n",
    "\n",
    "X_scaled_train, X__scaled_test, y_scaled_train, y_scaled_test = train_test_split(X_scaled, y, random_state=42)\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "grid = GridSearchCV(LogisticRegression(solver  = \"liblinear\", max_iter = 10000), param_grid, cv=5, scoring = \"f1\")\n",
    "grid.fit(X_scaled_train, y_scaled_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that rescaling the data matrix using TF-IDF while also removing stop words, together, did not improve the model's F1 score. This combination of preprocessing and tokenization methods performed the worst of the three attempts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part II: Build a predictive neural network using Keras\n",
    "\n",
    "*To complete part two of the homework do the following: Train test split the iris dataset and then run a multilayer perceptron (feed forward neural network) with two hidden layers on the iris dataset using the keras Sequential interface. Data can be imported via the following link: http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv fit two models with different numbers of hidden layers and or hidden neurons and evaluate each on a test-set.  Describe the differences in the predictive accuracy of models with different numbers of hidden units/neurons.  Describe the predictive strength of your best model.  Be sure to explain your choice and evaluate this model using the test set.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal.Length</th>\n",
       "      <th>Sepal.Width</th>\n",
       "      <th>Petal.Length</th>\n",
       "      <th>Petal.Width</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sepal.Length  Sepal.Width  Petal.Length  Petal.Width Species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "iris = pd.read_csv(\"http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv\") \n",
    "iris = iris.drop(iris.columns[[0]], axis = 1) # drop extra column that was added when data was read \n",
    "iris.head() "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
